{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2REY6Wkli6m",
        "outputId": "e770da9c-eb78-4715-a134-b4a8b6bdca24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers # Install transformers library"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is `transformers`(library)?\n",
        "\n",
        "- This is library that developed by [Hugging Face](https://huggingface.co/)(Company).\n",
        "- It's mainly used in NLP(Natural Language Processing) Fields.\n",
        "- Like its name(transformers), It is including many [Transformer](https://en.wikipedia.org/wiki/Transformer)-based model.\n",
        "- It can use with Pytorch, Tensorflow and other deep learning frameworks.\n",
        "- These days, it is used by not only NLP but also many fields like CV, Audio and etc.\n",
        "- It offers simple interface `pipeline`."
      ],
      "metadata": {
        "id": "_aVnrXjHmpBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2\n",
        "![GPT-2 Logo](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fstorage.googleapis.com%2Fwandb-production.appspot.com%2Fwandb-public-images%2Ftk8kfl2kbl.png&f=1&nofb=1&ipt=193d2f581835833fffada67a17cef7bd3a2768c559bb57d5047a4418eecce938)\n",
        "\n",
        "- GPT(Generative Pre-Trained Transformer) is a pre-trained model having **decoder-only** structure.\n",
        "- GPT-2 is second version of GPT model.\n",
        "- GPT-2 is published first time in [OpenAI](https://openai.com/) team's paper, [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever.\n",
        "- GPT-2 was trained by 'webtext'(private) dataset that consist of 40GB [Reddit](https://www.reddit.com/) posts that is received high evaluation.\n",
        "- GPT-2 was trained by **1.5B** parameters bigger than GPT-1's 170M parameters.\n",
        "- GPT-2's Zero-shot Learning has been very successful. GPT-2 can process various task by pre-trained status without fine-tuning."
      ],
      "metadata": {
        "id": "REqRu1v4pCIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-1 vs GPT-2\n",
        "- GPT-1(2018) and GPT-2(2019) are one generation apart. But they have a little difference in some parts.\n",
        "- The most difference is number of parameters. Because of parameters, GPT-2 could gain big performance elevation.\n",
        "\n",
        "The below is table that compared about GPT-1 vs GPT-2:\n",
        "\n",
        "|  | GPT-1 | GPT-2 |\n",
        "|---|---|---|\n",
        "| **Goal** | Fine-Tuning model with pre-trained based | Zero-shot Learning Performance |\n",
        "| **Model Size** | 117M parameters | **1.5B** parameters |\n",
        "| **Train Data** | BookCorpus(7,000 non-published books, 1GB) | webtext(Reddit posts, **40GB**) |\n",
        "| **Context Size** | 512 Tokens | **1024** Tokens |\n",
        "| **Vocabulary Size** | about 40,000 | about **50,257** |\n",
        "\n",
        "So, let's take a look more detail about between GPT-1 and GPT-2 difference.\n",
        "\n",
        "## Zero-shot Learning\n",
        "- Zero-shot Learning is a **methodology** that make model can process various tasks without fine-tuning.\n",
        "- We can't expect zero-shot learning to GPT-1. Because GPT-1 had an insufficient number of parameters, its ability to effectively learn and generalize across various tasks was limited.\n",
        "- But GPT-2 can process zero-shot learning, because GPT-2 1.5B parameters that is about 10x more than GPT-1's 117M parameters.\n",
        "- So, GPT-2 can process various task without fine-tuning.\n",
        "\n",
        "## A Slight Improvement about Model Architecture\n",
        "- GPT-2 architecture is also based on transformer decoder, but it is a little changes than GPT-1.\n",
        "  1. **Layer Normalization Location Changing**: GPT-2 architecture's most big changing is Layer Normalization location changing. In GPT-1's case, GPT-1 processed normalization after attention operation. This is called by **Post-LN**. This method can make learning unstable. So, GPT-2 adopted **Pre-LN** method that process normalization before attention operation. This was need even more to GPT-2 that has many number of transformer block.\n",
        "  2. **Context Size Improvement**: Context size is increase about twofold. From GPT-1's 512 tokens to GPT-2's 1024 tokens. So, model can understand more better about conversation's context."
      ],
      "metadata": {
        "id": "Qj6iqQNTBzEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 Structure\n",
        "![GPT-1 and GPT-2 Structure Diagram](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fpub-fb664c455eca46a2ba762a065ac900f7.r2.dev%2FGPT1_vs_GPT2_architecture.webp&f=1&nofb=1&ipt=79bce78eed066822b0f107e8740f3e63fdb8f37455304b22f4587d328bd7930b)\n",
        "\n",
        "To look GPT-2's Structure, we will use transformer library. Transformer libary support this."
      ],
      "metadata": {
        "id": "oOFl51IUB47l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Config\n",
        "\n",
        "# Python class that including configuration information about GPT2 Model\n",
        "# Configuartion\n",
        "# For the more details, follow 32 line in this link: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/configuration_gpt2.py#L31\n",
        "config = GPT2Config.from_pretrained('gpt2')\n",
        "\n",
        "# Print configuration informations\n",
        "print(config)"
      ],
      "metadata": {
        "id": "nAMbqQpvB6_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9c4b3fc-0bd7-445d-8901-087fc74a25f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.56.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2Config\n",
        "- We can see many informations for GPT2 Model.\n",
        "- For example, `n_ctx` is context vector size(1024). It's the same as what we've seen. And we can more informations like `activation_function`(GPT use gelu), `n_embd`(Embeding vector dimension), etc.\n",
        "- For a more inforamtion, you can see in this link: [transformer library source code in github repository](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2)"
      ],
      "metadata": {
        "id": "okhA9lw5kCaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Model\n",
        "\n",
        "# GPT-2 Model is python class that is model generated by GPT2Config and learend by webtext dataset\n",
        "model = GPT2Model.from_pretrained('gpt2')\n",
        "\n",
        "# Print model layer information\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPQvm1Z4cre6",
        "outputId": "f63e15cc-bd84-4d15-9f4b-fca6925cba5a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Model(\n",
            "  (wte): Embedding(50257, 768)\n",
            "  (wpe): Embedding(1024, 768)\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (h): ModuleList(\n",
            "    (0-11): 12 x GPT2Block(\n",
            "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): GPT2Attention(\n",
            "        (c_attn): Conv1D(nf=2304, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=768)\n",
            "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): GPT2MLP(\n",
            "        (c_fc): Conv1D(nf=3072, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=3072)\n",
            "        (act): NewGELUActivation()\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2Model\n",
        "- We can see a real architecture information about GPT2 Model.\n",
        "- For example, `LayerNorm` is located before `GPT2Atteion` in `GPT2Block`. It is same as what we've seen.\n",
        "- And GPT2Model use number of 12 `GPT2Blcok`s in GPT2Model.\n",
        "- And we can see some `Dropout` layers for preventing overfitting."
      ],
      "metadata": {
        "id": "kC2yxFzGlLPt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KK54ka_LlUUT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}