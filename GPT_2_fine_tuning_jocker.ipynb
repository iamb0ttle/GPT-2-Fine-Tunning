{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# JPT-2 (Jocker Pre-tarined and Tuned-Transformer)\n",
        "\n",
        "<img src=\"https://i.postimg.cc/GmnXj8Ld/Gemini-Generated-Image-3xdk643xdk643xdk.png\" width=\"300\" height=\"300\" alt=\"JPT-2 Logo\"><img/>\n",
        "\n",
        "# Overview\n",
        "- This project purpose is fine-tunning GPT-2 into JPT-2(Jocker Pre-tarined and Tuned-Transformer)\n",
        "- *Why so serious..?*\n"
      ],
      "metadata": {
        "id": "IvngURNzf7tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"jocker_lines.jsonl\")"
      ],
      "metadata": {
        "id": "Y-Cd_vWeh75o"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlL3jQ35lTzm",
        "outputId": "67ab7238-701e-4431-fe78-677f97e2f955"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 470\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset['train'].train_test_split(test_size=0.1, shuffle=True, seed=42)"
      ],
      "metadata": {
        "id": "i_3fLLKYlVUP"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_HZp1x-lgym",
        "outputId": "8827e51c-8860-48d2-fd9e-e0074c0d67c0"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 423\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 47\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0])\n",
        "print(dataset['test'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X22lboHjlkpM",
        "outputId": "392e1c39-51d5-46a9-9886-e5fef2acf145"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'I love the smell of fear in the morning. It smells like... victory.'}\n",
            "{'text': \"I'm not clumsy. I'm just a random act of violence waiting to happen.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # Make new token for padding"
      ],
      "metadata": {
        "id": "D90QtdE1yJ7V"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wmcsgRsny6D1",
        "outputId": "b9ad4be8-9df4-49f5-d705-768304af6667"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
        "\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenizer_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")"
      ],
      "metadata": {
        "id": "qoLIjwEBzij4"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDmDSRYv1lts",
        "outputId": "f004002f-52ed-4cfd-f09b-e2603ba89739"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 423\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask'],\n",
              "        num_rows: 47\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "base_model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxP_P4n_zCp0",
        "outputId": "b822bcde-56df-4dde-f1cf-8428666acaa8"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50258, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(base_model, lora_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiMsnSYX1yJC",
        "outputId": "15d7ff8f-181f-45a2-d8de-bd913e2d31b0"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MLk_0cG2YhX",
        "outputId": "7bf82a76-5f5f-4fd4-d3d2-1692a1c3410f"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 147,456 || all params: 124,588,032 || trainable%: 0.1184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"jpt2-lora\",\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        "    weight_decay=0.01\n",
        ")"
      ],
      "metadata": {
        "id": "JPl7Wh7r2ou2"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# data_collator make lable colunm from input_ids\n",
        "# It is play a same role with `result[\"labels\"] = result[\"input_ids\"].copy()`\n",
        "# And dobule check tokenized whether it is\n",
        "# `mlm=False` means that collator doens't maksing any tokens.\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "iDR_x19z3MBF"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL6RLsMh3ZT3",
        "outputId": "5ae6a71a-178f-49cd-8bd6-b1389ed1a583"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2353581448.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "APskWpvT3ldB",
        "outputId": "eeddeaf0-82eb-474e-f6ad-64fabacf20c0"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50257}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='212' max='212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [212/212 12:15, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.553508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.534223</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=212, training_loss=3.5659856256449, metrics={'train_runtime': 738.503, 'train_samples_per_second': 1.146, 'train_steps_per_second': 0.287, 'total_flos': 27679535529984.0, 'train_loss': 3.5659856256449, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model = lora_model.merge_and_unload()\n",
        "\n",
        "merged_model.save_pretrained(\"./jpt2\")"
      ],
      "metadata": {
        "id": "wOiMxa5u-q09"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, GPT2Tokenizer, pipeline\n",
        "\n",
        "jpt2 = AutoModelForCausalLM.from_pretrained(\"./jpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=jpt2, tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw1n9gg9_1v3",
        "outputId": "15df162f-09a6-477a-b2ee-37cfff41ee35"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Why so serious about this city?\"\n",
        "\n",
        "outputs = generator(prompt,\n",
        "                    max_new_tokens=64,\n",
        "                    num_return_sequences=3,\n",
        "                    do_sample=True,\n",
        "                    top_k=50)\n",
        "\n",
        "print(f\"--- Prompt: {prompt} ---\")\n",
        "for i, output in enumerate(outputs):\n",
        "    print(f\"Result {i+1}: {output['generated_text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za3LPFOOAh76",
        "outputId": "e403c5c8-f0dd-4015-e8fd-88d933854ef0"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Prompt: Why so serious about this city? ---\n",
            "Result 1: Why so serious about this city? I would love to be able to walk my dog, but it just doesn't seem to be very productive. I know that I'm not in a position to buy a house, but I feel that this city needs to move away from the current housing model that is the key to revitalizing the area. I want to\n",
            "Result 2: Why so serious about this city? Why do we have these people who are taking things into their own hands?\"\n",
            "\n",
            "\"I don't think so,\" said the young woman, who appeared to be more frightened than she was.\n",
            "\n",
            "\"The only things that can really save us,\" said the woman, \"are the people who are fighting back.\n",
            "Result 3: Why so serious about this city? Well, that depends on how you look at it.\n",
            "\n",
            "The city's population is about one million, and it's already well below the national average.\n",
            "\n",
            "That means, of course, that it's not really the biggest city in the country, but it's still one of the most diverse.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result\n",
        "- As you saw above, I failed to mimic joker character's parlance.\n",
        "- To improve output, I even applied priming method in prompt like \"Why so serious about this city?\".\n",
        "- But through this change, I could see many what problems happen when model collapse.\n",
        "  1. **Not working as intended**: Most general problem. Naturally, the learning fails and we cannot expect the desired (joker-speak) outcome, and is almost identical to the output of the pre-fine-tuning model\n",
        "  2. **Model Collapes**: The model is trapped in a specific pattern to reduce the loss.\n",
        "\n",
        "## Why this project failed?\n",
        "Here is some expected problems about fail.\n",
        "1. **Lack of data**: To process project, I had to gain jocker-speech dataset. But it was difficult, So I made dataset with using 'Gemini-2.5-pro'. But data was still lack and low-quality. So, model couldn't understand jocker-speeech distinction in learning.\n",
        "2. **Model Size**: 1.5B parameters is very bigger than GPT-1's 117M. But, it is not sufficient to learn jocker-speech distinction. Jocker speech distinction is very ambiguous and sophisticated. So, if model parameter size was big, we may have had better results.\n",
        "3. **Prompt**: In the first test, I input \"City\" in prompt. But, \"City\" is very general and predictable word. So, outputs were very general and predictable. But after apply priming, output more imporvement.\n",
        "\n",
        "# Final Toughts\n",
        "This project was a practical lesson in the challenges of fine-tuning a persona. Our attempt was ultimately constrained by a combination of three factors: a dataset too small to convey the character's complexity, a model (GPT-2) whose scale was insufficient to override its powerful pre-training, and the clear necessity of specific, guiding prompts to activate the new style. This demonstrates that creating a convincing persona is a careful balance between the quality of data, the capacity of the model, and the art of the prompt."
      ],
      "metadata": {
        "id": "mMLoF2CbH6Zw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2UCBTZsfOMzv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
