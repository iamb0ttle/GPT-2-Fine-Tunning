{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 Fine-Tuning Model with New Dataset: GPT-2News\n",
        "\n",
        "![Breaking News](https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fstatic.dnaindia.com%2Fsites%2Fdefault%2Ffiles%2Fstyles%2Ffull%2Fpublic%2F2017%2F06%2F02%2F580712-breaking-news.jpg&f=1&nofb=1&ipt=98058e9d5b86187d7f576d67f85f612bdd58c3b5ca30bb60bca7750b4846be90)\n",
        "\n",
        "# Overview\n",
        "- This Project purpose is fine-tuning GPT-2 with LoRA method.\n",
        "- I used [fancyzhx/ag_news](https://huggingface.co/datasets/fancyzhx/ag_news) dataset to train model into model that mimic news style.\n"
      ],
      "metadata": {
        "id": "ji-eNvq3saC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# datasets: hugging face libary for open datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "# load data set from specific hugging face hugging face\n",
        "# this will return `DatasetcDict` class\n",
        "dataset = load_dataset(\"fancyzhx/ag_news\")"
      ],
      "metadata": {
        "id": "YAn2DjMgswb3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset # check dataset information"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNOwG8PSt4Ez",
        "outputId": "5ca210cf-8fb3-45ac-b12b-0c840d091c37"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 120000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 7600\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict # DatasetDict class import\n",
        "\n",
        "# We don't have to use all of dataset for fine-tunning, we just datasets about 2000 ~ 3000\n",
        "# So, we will shuffle(for a more various) and select some datasets\n",
        "train_small = dataset[\"train\"].shuffle(seed=42).select(range(2000))\n",
        "valid_small = dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
        "\n",
        "# Make new datasetdict\n",
        "small_dataset = DatasetDict({\n",
        "  \"train\": train_small,\n",
        "  \"validation\": valid_small\n",
        "})"
      ],
      "metadata": {
        "id": "smyYLridu92u"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(small_dataset) # check new dataset information"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nl_G6qYwWxr",
        "outputId": "51e886f8-f350-4766-daef-f57f3c162508"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load GPT2Tokenizer from pre-tranied GPT2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set up pad token to padding. GPT2 doesn't have pad token, because it is casual LM.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# make tokenizer function for datasetdict\n",
        "def tokenize_function(examples):\n",
        "  # `truncation = True` means if number of token is longger than max_length, model will cut tokens until 128.\n",
        "  # `padding = max_length` means if number of token is shorter than max_length, model will fill empty space by padding token.\n",
        "  # `max_length = 128` means token maximum size.\n",
        "  # For a fast tunning and learning, I restrict max token just 128.\n",
        "  result = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "  # make labels colunm model can caculate loss and update weight\n",
        "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "wbRDSgg9xFMm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make new Datasetdict object that is tokenized by tokenize_function(use GPT2 tokenizer).\n",
        "# `DatasetDict.map(x: def)` function apply function `x` every row in `DatasetDict.Dataset`s.\n",
        "# `batched=True` means `x` function apply with batch.\n",
        "# `remove_columns=[\"text\", \"label\"]` means delete colums in dataset.\n",
        "tokenized_datasets = small_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\", \"label\"])\n",
        "\n",
        "# new datasets has two colums `input_ids` and `attention_mask`.\n",
        "# `input_ids`: row number for vector that specific word vectorized.\n",
        "# `attention_mask`: 0 and 1 masking vector to indicate which vector have to use in attention\n",
        "# -------- colums example ----------\n",
        "# | input_ids |\tattention_mask | | labels |\n",
        "# | ---- | ---- | ---- |\n",
        "# | [101, 12, 18, ...] |\t[1, 1, 1, 1, ...] | [101, 12, 18, ...] |\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WMk2gl_zLoy",
        "outputId": "5cc11d18-af79-4709-e4da-1e6468a33836"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model # import LoRA related classes from PEFT(Parameter-Efficient Fine-Tuning) libray\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# import GPT2 casual LM version as pre-trained\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# to use LoRA(Low-Rank Adaptation) method, we can use LoraConfig class for set LoRA parameters\n",
        "lora_config = LoraConfig(\n",
        "  r=8, # rank in this LoRA. to be efficient, I set low rank slightly\n",
        "  lora_alpha=16, # lora_alpha means adapter matrix's influence, (lora_alpha / r) is magnifying power that decide how many focus on LoRA adapter\n",
        "  target_modules=[\"c_attn\"], # c_attn(Cross-Attention Layer) is most efficient layer in transformer. It was mentioned in paper, table 5 of 'LoRA: Low-Rank Adaptation of Large Language Models'(2021).\n",
        "  lora_dropout=0.05, # set percentage of lora dropout. I set 5%. It think that is very suitable.\n",
        "  bias=\"none\", # doesn't find new bias. ues original bias. Our purpose is sufficient only updating weights.\n",
        "  task_type=\"CAUSAL_LM\" # set as `CASUAL_LM` to work same as GPT2\n",
        ")\n",
        "\n",
        "# make new peft model with LoraConfig class from GPT2\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG6VeNw064Ml",
        "outputId": "2436a8cc-bdd0-4d07-ff16-0a5bb77f64bd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer # classes for transformer model's learning\n",
        "\n",
        "# TrainingArguments class is for containing argumetns that will use in training\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./results\", # directory that will be stored model that is completed training.\n",
        "  # set when will evaluate model evaluation. 'ephoc' means when ephoc done, model will be evaluated.\n",
        "  # we can use parameters like 'no', 'ephoc' or 'step' in `eval_strategy`\n",
        "  eval_strategy=\"epoch\",\n",
        "  learning_rate=2e-4, # learning late to update LoRA adapter weight. It is setted more bigger value than full-weight fine-tunning\n",
        "  per_device_train_batch_size=4, # train batch size\n",
        "  per_device_eval_batch_size=4, # test batch size\n",
        "  num_train_epochs=5, # we will prcoess 5 ephocs\n",
        "  weight_decay=0.01, # to prevent overfitting, set weight decay percentage as 1%.\n",
        "  logging_dir=\"./logs\", # directory that will be stored rate of loss changing.\n",
        "  save_strategy=\"no\", # we don't save check point model\n",
        "  report_to=\"none\" # doesn't use third-party for loss tracking\n",
        ")\n",
        "\n",
        "# Trainer class is for training specific model with taring arguments\n",
        "trainer = Trainer(\n",
        "  model=model, # target model we will tarin\n",
        "  args=training_args, # tarining arguments\n",
        "  train_dataset=tokenized_datasets[\"train\"], # tarin datasets\n",
        "  eval_dataset=tokenized_datasets[\"validation\"], # test datasets\n",
        ")"
      ],
      "metadata": {
        "id": "Uf_85GbE0mfw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train() # train model"
      ],
      "metadata": {
        "id": "vGHg69iuDyDg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "12b7b18d-a6d8-4626-c751-bf81100288b0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 05:25, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.037200</td>\n",
              "      <td>1.584247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.595200</td>\n",
              "      <td>1.531527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.550700</td>\n",
              "      <td>1.503057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.528300</td>\n",
              "      <td>1.491394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.516200</td>\n",
              "      <td>1.488445</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=1.64554443359375, metrics={'train_runtime': 325.4463, 'train_samples_per_second': 30.727, 'train_steps_per_second': 7.682, 'total_flos': 655495004160000.0, 'train_loss': 1.64554443359375, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model = model.merge_and_unload() # make new merged model that is merged LoRA updater\n",
        "\n",
        "merged_model.save_pretrained(\"./gpt2-news-merged\") # save model in storage"
      ],
      "metadata": {
        "id": "3Ja22xZuFZLk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, GPT2Tokenizer, pipeline\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./gpt2-news-merged\") # load model with using AutoTransformer class\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") # Tokenizer is same GPT2's\n",
        "\n",
        "# generate generator with using pipeline\n",
        "generator = pipeline(\"text-generation\", # task: text-generation\n",
        "                     model=model, # model = gpt2-news-merged\n",
        "                     tokenizer=tokenizer) # translate `input_ids` into natural language. with using this tokenizer(GPT2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJF613zRJfbK",
        "outputId": "7594780b-c732-4e50-a253-bbb0bf9c5833"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Breaking News\" # prompt. model will be generating from this sentence\n",
        "\n",
        "outputs = generator(prompt, # prompt\n",
        "                    max_length=128, # max token length\n",
        "                    num_return_sequences=3, # number of generting sequence\n",
        "                    do_sample=True, # sampling true\n",
        "                    top_k=50) # sampling only for top 50 vector\n",
        "\n",
        "# print generated outputs\n",
        "for i, out in enumerate(outputs):\n",
        "    print(f\"--- Generated #{i+1} ---\")\n",
        "    print(out[\"generated_text\"])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2UH_qfaKRg9",
        "outputId": "d97f4e9a-db91-4bcb-f761-da070933a2e9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generated #1 ---\n",
            "Breaking News: New York Times Is In The Business Of Selling Paper for Profit\n",
            "\n",
            "--- Generated #2 ---\n",
            "Breaking News: A New York Times Reporter Calls for President to 'Make America Great Again' AP A New York Times reporter called for President Donald Trump to \"make America great again\" on Monday, tweeting the slogan # #AmericaFirst.\n",
            "\n",
            "--- Generated #3 ---\n",
            "Breaking News & Rumors A report from The Daily Mail has claimed that former Conservative MP David Cameron has been suspended from the party after a controversial speech in which he accused the party of being anti-Islamic.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 vs GPT-2**News**"
      ],
      "metadata": {
        "id": "3rhHs-pBQr0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, AutoModelForCausalLM, GPT2Tokenizer, pipeline\n",
        "\n",
        "# ---------- GPT2 ------------\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# ---------- GPT2News ----------\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./gpt2-news-merged\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2news_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# --------- Test --------------\n",
        "prompt = \"Seoul, South Korea\"\n",
        "\n",
        "gpt2_outputs = gpt2_generator(prompt, max_length=128, num_return_sequences=3, do_sample=True, top_k=50)\n",
        "gpt2news_outputs = gpt2news_generator(prompt, max_length=128, num_return_sequences=3, do_sample=True, top_k=50)\n",
        "\n",
        "# 1. ÏõêÎ≥∏ GPT-2Ïùò ÏÉùÏÑ± Í≤∞Í≥º Ï∂úÎ†•\n",
        "print(\"=============================================\")\n",
        "print(\"          ü§ñ Original GPT-2 Results          \")\n",
        "print(\"=============================================\")\n",
        "print(f\"PROMPT: '{prompt}'\\n\")\n",
        "\n",
        "for i, output in enumerate(gpt2_outputs):\n",
        "    print(f\"--- Generated #{i+1} ---\")\n",
        "    # ÌååÏù¥ÌîÑÎùºÏù∏ Í≤∞Í≥ºÎäî ÎîïÏÖîÎÑàÎ¶¨ ÌòïÌÉúÏù¥ÎØÄÎ°ú 'generated_text' ÌÇ§Î°ú Ï†ëÍ∑ºÌï©ÎãàÎã§.\n",
        "    print(output['generated_text'])\n",
        "    print() # Ï§ÑÎ∞îÍøà\n",
        "\n",
        "# 2. ÌååÏù∏ÌäúÎãùÎêú GPT-2 News Î™®Îç∏Ïùò ÏÉùÏÑ± Í≤∞Í≥º Ï∂úÎ†•\n",
        "print(\"\\n=============================================\")\n",
        "print(\"      ‚ú® Fine-tuned 'GPT-2 News' Results      \")\n",
        "print(\"=============================================\")\n",
        "print(f\"PROMPT: '{prompt}'\\n\")\n",
        "\n",
        "for i, output in enumerate(gpt2news_outputs):\n",
        "    print(f\"--- Generated #{i+1} ---\")\n",
        "    print(output['generated_text'])\n",
        "    print() # Ï§ÑÎ∞îÍøà"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t22znEjQyO7",
        "outputId": "b6906574-8073-4ab2-e579-68e9a86e75bc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================\n",
            "          ü§ñ Original GPT-2 Results          \n",
            "=============================================\n",
            "PROMPT: 'Seoul, South Korea'\n",
            "\n",
            "--- Generated #1 ---\n",
            "Seoul, South Korea, and South Korea are considered the most important oil reserves in the world. However, many of the countries that rely on oil for their livelihood are also rich in natural resources, including oil from the Bakken, which is a huge, natural resource.\n",
            "\n",
            "The United States is the world's largest producer of oil, with a combined total of about 2.3 billion barrels of oil. The United States produces about 25 percent of the world's oil, and has a significant share of the global population. However, the United States does not have the largest reserves of oil, and only about 20 percent of its oil is produced in the country that produces it.\n",
            "\n",
            "In 2014, the U.S. reported a total of $1.7 trillion in assets abroad, or nearly $1.5 trillion in the United States. The United States relies on the Bakken for its oil production, which is expected to reach $100 billion by 2020.\n",
            "\n",
            "The United States maintains its natural resources at the Bakken. Natural Resources Secretary Ernest Moniz said the US does not have an oil reserve in North America, and it will continue to rely on the Bakken for the production of its natural resources.\n",
            "\n",
            "In March, the U.S. Energy Information Administration (\n",
            "\n",
            "--- Generated #2 ---\n",
            "Seoul, South Korea, and the Philippines have said they would take back the North Korean nuclear program if it were to go ahead. North Korea's own test-fire of a hydrogen bomb on a South Korean island on June 8 has raised questions about its ability to test nuclear weapons.\n",
            "\n",
            "The United Nations Security Council has called on the country to stop its nuclear program.\n",
            "\n",
            "U.N. officials said the United States and South Korea have agreed to send a representative to the region to address the North Korean nuclear deal, as well as to discuss the possibility of conducting diplomatic talks with the North.\n",
            "\n",
            "But the U.S., South Korea and Japan have refused to meet with the North's leader, Kim Jong Un, in recent days, and the U.S. has voiced concern that North Korea may attempt to test a hydrogen bomb, prompting renewed calls for an early meeting with the leader.\n",
            "\n",
            "\"There's nothing we can do to delay a meeting with the leader to avoid a confrontation with North Korea,\" said a U.N. diplomat who spoke on the condition of anonymity to discuss internal internal deliberations.\n",
            "\n",
            "\"We want a quick, short-term deal and a long-term, successful cessation of hostilities,\" the diplomat said. \"There is no other way to accomplish\n",
            "\n",
            "--- Generated #3 ---\n",
            "Seoul, South Korea, September 9, 2017. REUTERS/Yonhap\n",
            "\n",
            "The first U.S. nuclear deal is due to take effect next month.\n",
            "\n",
            "\"The U.S. nuclear program will continue to be a top priority for the United States, and we have signed an agreement to continue our partnership,\" a senior U.S. official said.\n",
            "\n",
            "\"This is a good sign for North Korea, and the U.S. also hopes that this agreement will spur the DPRK to improve its relationship with the United States,\" the official said.\n",
            "\n",
            "The U.S. nuclear deal with North Korea has been under scrutiny by U.S. President Donald Trump and his administration.\n",
            "\n",
            "China and Russia have said they were not aware of the North Korea agreement and South Korea and Japan have denied any such talks were ever taking place.\n",
            "\n",
            "Trump on Thursday said he would sign the nuclear deal if the U.S. did not meet the terms.\n",
            "\n",
            "The pact is aimed at curbing North Korea's nuclear program and is the first U.S. deal to be signed since Pyongyang took over the Korean peninsula in 1953.\n",
            "\n",
            "North Korea said its leaders would \"break the ice\" in negotiations, and Trump's new administration would take the lead in\n",
            "\n",
            "\n",
            "=============================================\n",
            "      ‚ú® Fine-tuned 'GPT-2 News' Results      \n",
            "=============================================\n",
            "PROMPT: 'Seoul, South Korea'\n",
            "\n",
            "--- Generated #1 ---\n",
            "Seoul, South Korea, to Seek Nuclear Weapons for South Korean Defence Force (AP) AP - South Korea and North Korea are considering a possible use of nuclear weapons for defence purposes, Yonhap reported on Sunday, citing the South Korean defence ministry.\n",
            "\n",
            "--- Generated #2 ---\n",
            "Seoul, South Korea - The U.N. Security Council has issued a statement calling for South Korea to halt its nuclear program and called for the United States and South Korea to stop their nuclear activities, according to a U.N. official.\n",
            "\n",
            "--- Generated #3 ---\n",
            "Seoul, South Korea - A South Korean military plane was reportedly involved in a crash on Friday, killing one person and injuring five more, South Korea's Yonhap news agency reported.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
