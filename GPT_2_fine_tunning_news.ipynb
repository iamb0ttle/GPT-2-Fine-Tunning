{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 Fine-Tunning Model with New Dataset: GPT-2News\n",
        "\n",
        "![Breaking News](https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fstatic.dnaindia.com%2Fsites%2Fdefault%2Ffiles%2Fstyles%2Ffull%2Fpublic%2F2017%2F06%2F02%2F580712-breaking-news.jpg&f=1&nofb=1&ipt=98058e9d5b86187d7f576d67f85f612bdd58c3b5ca30bb60bca7750b4846be90)\n",
        "\n",
        "# Overview\n",
        "- This Project purpose is fine-tuning GPT-2 with LoRA method.\n",
        "- I used [fancyzhx/ag_news](https://huggingface.co/datasets/fancyzhx/ag_news) dataset to train model into model that mimic news style.\n"
      ],
      "metadata": {
        "id": "ji-eNvq3saC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# datasets: hugging face libary for open datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "# load data set from specific hugging face hugging face\n",
        "# this will return `DatasetcDict` class\n",
        "dataset = load_dataset(\"fancyzhx/ag_news\")"
      ],
      "metadata": {
        "id": "YAn2DjMgswb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "c1791726e35648819d628d78facbf6f3",
            "8cf6ead638d04e88b542a21fbb7c1052",
            "4bb9cc284248414ca7597b3261f7c414",
            "8feb574ae2cf42bc889d87c7681b6068",
            "27ca8296c9084de5995d300ff3b36fa2",
            "95fe212f41ea44cab7b56e33385fe5f9",
            "9fca8a1a537246ccab842b6adacf4c83",
            "482deeb380004ba1877b572115ffde7d",
            "e47d4552152345fcb34f7f925775e4b2",
            "23eb7fa950d04ff9a78573f6dc64167d",
            "0a02b384fc8e4fedbf57473c8793b1c7",
            "f93d724752cb431b99ced3347ecf38cb",
            "b619e2413dd04ddeaffdf31a5d3d6f34",
            "a03b2bb0c5e4472eaf78a7a15df5b75b",
            "0b019b14f51c4d46a9382b840226ab7b",
            "f89ef5c8a5b84c448b4708d35e4f1c7b",
            "4796041edbac4b1eb4c35a0f10c21729",
            "92212b7221284d879ed21da0a4d99020",
            "dc29fd4379ed44f8a6f5d05d638fe3dc",
            "1af5c1c4f85546448bdddc287d3efbe4",
            "0c87f67f89054345a74756cb4ea896da",
            "060d793e1372469fb296be5fbbe4d3c7",
            "ade8990526aa439a97bd69f0ae84ae0c",
            "603157f7b2d3494a8c70af9bca119797",
            "192ddc0d353545279971857cc705649e",
            "29e8be94fdad404998008c5f70e6dddd",
            "0891c17a90104216978f7401a734b6f0",
            "769b4f3edff349d1b5292b9a46ef2c66",
            "a604dbc13ea844c1b5148b69086594cd",
            "738678cbb2c14088825f21f20aa7aaa6",
            "40b6c2c1542c4114b2399f6096bd6063",
            "c9e7ce33b47a49ca886ac7c840e19281",
            "2fc66b64ae7b4efc927b1daba450328b",
            "4b1e881a6e2d4ddd88add2df35a0b048",
            "5d4715c0046e4c6e9eb897832dd3f327",
            "24ee421714884aa991050da052f72149",
            "3a766dc712c44844b1baabf8803b86b3",
            "a64b1a63cde54e64a26df37c520cebd7",
            "f4480efa88c142c09bc01c221b756fa3",
            "27dde42e2a4f4bbb8d5651ec5b3f567e",
            "2f2dad0911684eaaba0a82961a825072",
            "b06d68c7cb7e42a9bfdd5ffeea304804",
            "5e8d61611d7f40648b0d210054070085",
            "2d0395a8fd464ca5bc41541a0366a12b",
            "989867feeca04437a77e9b017ff798da",
            "5f4183716a144783beea77d7a54757e1",
            "15501630ed184c3d9fa606bf8a1b4b16",
            "a734d810923f4f79a6c37020296a6961",
            "ecbc5f25e5fa44f78ee83c575430fbb4",
            "5b75b13b6a5947539481b01ef295496b",
            "959d0b46ef5440ff9953382545f53190",
            "5a447b4b59474feaa409e64d5b4b7695",
            "718a1c8e759e41b98d07d6cac4069cfc",
            "6c91c6c54fad446982e7bc4aecc69668",
            "aa7e6b4355674e6abd8f940170bc7bd9"
          ]
        },
        "outputId": "b9d5c6e7-6357-4ec3-b518-6e9773e35428"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1791726e35648819d628d78facbf6f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f93d724752cb431b99ced3347ecf38cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ade8990526aa439a97bd69f0ae84ae0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b1e881a6e2d4ddd88add2df35a0b048"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "989867feeca04437a77e9b017ff798da"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset # check dataset information"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNOwG8PSt4Ez",
        "outputId": "1325ef9d-f917-45c2-b4ac-25f4bf0293be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 120000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 7600\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict # DatasetDict class import\n",
        "\n",
        "# We don't have to use all of dataset for fine-tunning, we just datasets about 2000 ~ 3000\n",
        "# So, we will shuffle(for a more various) and select some datasets\n",
        "train_small = dataset[\"train\"].shuffle(seed=42).select(range(2000))\n",
        "valid_small = dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
        "\n",
        "# Make new datasetdict\n",
        "small_dataset = DatasetDict({\n",
        "  \"train\": train_small,\n",
        "  \"validation\": valid_small\n",
        "})"
      ],
      "metadata": {
        "id": "smyYLridu92u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(small_dataset) # check new dataset information"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nl_G6qYwWxr",
        "outputId": "66e25d2f-d130-4c5a-ea98-b2abefbecfcb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load GPT2Tokenizer from pre-tranied GPT2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set up pad token to padding. GPT2 doesn't have pad token, because it is casual LM.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# make tokenizer function for datasetdict\n",
        "def tokenize_function(examples):\n",
        "  # `truncation = True` means if number of token is longger than max_length, model will cut tokens until 128.\n",
        "  # `padding = max_length` means if number of token is shorter than max_length, model will fill empty space by padding token.\n",
        "  # `max_length = 128` means token maximum size.\n",
        "  # For a fast tunning and learning, I restrict max token just 128.\n",
        "  result = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "  # make labels colunm model can caculate loss and update weight\n",
        "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "wbRDSgg9xFMm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "297cf436910a45e6a3cbc3c872d815eb",
            "68394a061d69421bbaf476522ac7ef6f",
            "0e7cab9f1f314e51bfd7ea2d5eb95c96",
            "aff4d4cdee71435aa49f77a51f68dc84",
            "cb92f6e7281d4d4a8a257a869ab3e89c",
            "f38d03dee37f4473a5f48ba47055f0e4",
            "671d0acbad924e6b97b8ed8efb277b7d",
            "aa1723fd9af34821b603e21b409fe874",
            "496f430074c94d609a9d015a06859e52",
            "bf55d6a2cd144830993c0ed73a909ad5",
            "07ffe5a5e55f475b9d4c6fbc4d42b8ea",
            "fac193deb69f48d5a9a8c9cecb8ce288",
            "13322c19f8574164a381035f1016794a",
            "513d3016ccc44e35b14f509a61b35a5b",
            "1a7dc19918ad4ed5845928ad24f4592b",
            "28c7fd87d83943e284cfe4e12fd3b6ff",
            "e544a7538b3f477d8cbdc1c669eac7a3",
            "f74a9b05c0194faa8a8bfe4bde4309ff",
            "1df1f7d8d25c425bb230e5e6ed7bff72",
            "c4fef92838cf4ff4b5e940d756b1d28f",
            "3264756e563a4a2eaffea09fc6c43750",
            "a9eb9a02bdc84d7e8f112756e1c0fdbb",
            "5b21ad5c370c4b278e11fd48b7798c62",
            "0900ad6feaf6499ab94c229c63d2d128",
            "b3d857a6a99a472ea45b5fb48ae1975c",
            "e763c8ed1eb64eb8a4a3479cb261eac2",
            "a8924ec0d67740efa4f26564e27175c5",
            "3fca5e1675e84b0382ec94d61f662994",
            "860d4b7a50124b31a3dea48579e0024e",
            "05b624d924b143bf9fa6b2117abfeaad",
            "dd0f0867a5304c279e402d5701af4baa",
            "965da2a4117c4b339236207c21e86ed7",
            "570d634d282a4b1ab4a8b9b9dbbde733",
            "2c99690e99a9456a9f5c9b1c1c0d14ba",
            "323436a9fbfe42e9a6bf8a127f93c706",
            "cde83a667f9f4e8fa997a3aeb7bc0961",
            "7678ff6a445d4f1f9d99a5289762fca6",
            "68c1dec8ae8842be90a83df3fb312565",
            "a54d76b4fec74ab4926754bfdde0c7fe",
            "a66a5b118d2e4e9c9327cf91901c8492",
            "4db7f9910b45490793cd9f5d5ab57023",
            "97ac8d17f0cc4f3caa5162bdd36cd534",
            "f8ec7893a6184ce59cd963861ac95fcc",
            "1301ae09fcff483db88c86dfd7cda153",
            "2aa5d9c6d5c645868d484e87fa0f614b",
            "848d7adbe0f44d83a09281c0edd095f9",
            "bc9748e970e14b1c96ea26f06f98be37",
            "11351e530f2f408cb48975350cfe02a3",
            "0f4d2764cb1745a68e90dbc20607a82d",
            "87d361143cf5480db29065c81059197f",
            "2768885762d04e14a9d62a8a4e7d9a8d",
            "7e6a14a0333d44b7923ec31fab12df96",
            "31825c70a196455db200f58a7a5511a7",
            "3002d900dcc549cab1c3749c2065cc7d",
            "68f3f7b6eab440ba99b69c9a1409deeb"
          ]
        },
        "outputId": "8eb264be-5569-4a8f-eeee-427496cbc628"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "297cf436910a45e6a3cbc3c872d815eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fac193deb69f48d5a9a8c9cecb8ce288"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b21ad5c370c4b278e11fd48b7798c62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c99690e99a9456a9f5c9b1c1c0d14ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2aa5d9c6d5c645868d484e87fa0f614b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make new Datasetdict object that is tokenized by tokenize_function(use GPT2 tokenizer).\n",
        "# `DatasetDict.map(x: def)` function apply function `x` every row in `DatasetDict.Dataset`s.\n",
        "# `batched=True` means `x` function apply with batch.\n",
        "# `remove_columns=[\"text\", \"label\"]` means delete colums in dataset.\n",
        "tokenized_datasets = small_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\", \"label\"])\n",
        "\n",
        "# new datasets has two colums `input_ids` and `attention_mask`.\n",
        "# `input_ids`: row number for vector that specific word vectorized.\n",
        "# `attention_mask`: 0 and 1 masking vector to indicate which vector have to use in attention\n",
        "# -------- colums example ----------\n",
        "# | input_ids |\tattention_mask | | labels |\n",
        "# | ---- | ---- | ---- |\n",
        "# | [101, 12, 18, ...] |\t[1, 1, 1, 1, ...] | [101, 12, 18, ...] |\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255,
          "referenced_widgets": [
            "b013ef7b55ad48ffb076af8339181e04",
            "dc8b62e651a6439a815cf2e118b95f9a",
            "5fcc813e8d7644b7a067c92ce1078e6d",
            "b6905d6af1d44b3e86ef70e0dacada59",
            "ef406a52f275451fb18ff1c5e368d2bb",
            "0d6ad9923b6e4ebebce92369d5ddc1e6",
            "78a64510e88340afbff7cc0094cb3998",
            "1184a594985f4ccbaee24440330d22b5",
            "10b2b05a3d594b36960ab9e01a1b1cae",
            "a75c03e15c09489d9bc724a37ec4a57e",
            "bda3775f764e43ccb209125503d2495a",
            "e6c8b6dd4a1742dd9921b9d46c97dcf1",
            "156b94ce05954c2d90fda645e68ca988",
            "fb41fa23b3894931b8e9c15796bd4ae9",
            "971c72da26e54190a70b74a3ff3f3328",
            "2edefe3c99e444198dea772fcb0ca4de",
            "fe908422aaaa4d32b35998461ce66643",
            "acad93aa307e44ef84c8ecef4b9bc6ff",
            "4ff41cc140064d8b85d4cf7724b9bdae",
            "cf053bfd05a140948e9336bdcc2b0d8f",
            "0d0ffa1823774eb58a88b5bf5f1e7a1c",
            "091d31efd40d444a9aa9c275d08c6990"
          ]
        },
        "id": "8WMk2gl_zLoy",
        "outputId": "46b262c7-bf5a-4dee-aa64-084146faeff1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b013ef7b55ad48ffb076af8339181e04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6c8b6dd4a1742dd9921b9d46c97dcf1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model # import LoRA related classes from PEFT(Parameter-Efficient Fine-Tuning) libray\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# import GPT2 casual LM version as pre-trained\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# to use LoRA(Low-Rank Adaptation) method, we can use LoraConfig class for set LoRA parameters\n",
        "lora_config = LoraConfig(\n",
        "  r=8, # rank in this LoRA. to be efficient, I set low rank slightly\n",
        "  lora_alpha=16, # lora_alpha means adapter matrix's influence, (lora_alpha / r) is magnifying power that decide how many focus on LoRA adapter\n",
        "  target_modules=[\"c_attn\"], # c_attn(Cross-Attention Layer) is most efficient layer in transformer. It was mentioned in paper, table 5 of 'LoRA: Low-Rank Adaptation of Large Language Models'(2021).\n",
        "  lora_dropout=0.05, # set percentage of lora dropout. I set 5%. It think that is very suitable.\n",
        "  bias=\"none\", # doesn't find new bias. ues original bias. Our purpose is sufficient only updating weights.\n",
        "  task_type=\"CAUSAL_LM\" # set as `CASUAL_LM` to work same as GPT2\n",
        ")\n",
        "\n",
        "# make new peft model with LoraConfig class from GPT2\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "514b4666891048ef951d3d8916898deb",
            "4edeed06b281463db2699e3c9664d3b1",
            "a002f920ffdb4af99306fbcfd484c7ed",
            "fac950b792a347ae9af47fa2e6a912f1",
            "81da16ddb7c84c79bec43883224b8008",
            "d1d25e1905204c35a2dd43abd514b8f8",
            "8eda06ceadca48ef9f81d01d77ef3f27",
            "b0bb1c9edc0140ca81d7752b4ffa34b0",
            "f9e29bbecc204b8780e57076b160d161",
            "057a70ec8c104cdda92a6ed58866f071",
            "60fe18bc03234d4e9c15e8cff728e485",
            "7807e4ed72e844b3aa3455dd3e4972cd",
            "8a6372eaf4bc4b3e88d70dff43dd53e7",
            "0db45d6f6179454892519402ab105d43",
            "70efae2019ef4618a1dbaa215f133034",
            "efe2ddae7b9949c7a0064dec36d3da72",
            "828e4f5041b24d659941d76b97a36f30",
            "f4d3ba3d47d941cda53ed3217ba8752c",
            "0b1f5697df3549098cdb3b8fcb81aba2",
            "54cdd1d517104dbd9c99ff1a45b858ff",
            "f67a0bd20aad455d9aeee0eb93dd417d",
            "a1fe51bf248e4f21a566d3fd603dce4c"
          ]
        },
        "id": "TG6VeNw064Ml",
        "outputId": "20c041b6-ca00-48a6-ba8c-d0dfea7c402e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "514b4666891048ef951d3d8916898deb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7807e4ed72e844b3aa3455dd3e4972cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer # classes for transformer model's learning\n",
        "\n",
        "# TrainingArguments class is for containing argumetns that will use in training\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./results\", # directory that will be stored model that is completed training.\n",
        "  # set when will evaluate model evaluation. 'ephoc' means when ephoc done, model will be evaluated.\n",
        "  # we can use parameters like 'no', 'ephoc' or 'step' in `eval_strategy`\n",
        "  eval_strategy=\"epoch\",\n",
        "  learning_rate=2e-4, # learning late to update LoRA adapter weight. It is setted more bigger value than full-weight fine-tunning\n",
        "  per_device_train_batch_size=4, # train batch size\n",
        "  per_device_eval_batch_size=4, # test batch size\n",
        "  num_train_epochs=5, # we will prcoess 5 ephocs\n",
        "  weight_decay=0.01, # to prevent overfitting, set weight decay percentage as 1%.\n",
        "  logging_dir=\"./logs\", # directory that will be stored rate of loss changing.\n",
        "  save_strategy=\"no\", # we don't save check point model\n",
        "  report_to=\"none\" # doesn't use third-party for loss tracking\n",
        ")\n",
        "\n",
        "# Trainer class is for training specific model with taring arguments\n",
        "trainer = Trainer(\n",
        "  model=model, # target model we will tarin\n",
        "  args=training_args, # tarining arguments\n",
        "  train_dataset=tokenized_datasets[\"train\"], # tarin datasets\n",
        "  eval_dataset=tokenized_datasets[\"validation\"], # test datasets\n",
        ")"
      ],
      "metadata": {
        "id": "Uf_85GbE0mfw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train() # train model"
      ],
      "metadata": {
        "id": "vGHg69iuDyDg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "a1fb583a-4d62-46b7-f922-55a429b9cb63"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 05:17, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.044700</td>\n",
              "      <td>1.585969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.595700</td>\n",
              "      <td>1.530692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.550100</td>\n",
              "      <td>1.501412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.527500</td>\n",
              "      <td>1.490116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.515800</td>\n",
              "      <td>1.487668</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=1.646754296875, metrics={'train_runtime': 318.2826, 'train_samples_per_second': 31.419, 'train_steps_per_second': 7.855, 'total_flos': 655495004160000.0, 'train_loss': 1.646754296875, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model = model.merge_and_unload() # make new merged model that is merged LoRA updater\n",
        "\n",
        "merged_model.save_pretrained(\"./gpt2-news-merged\") # save model in storage"
      ],
      "metadata": {
        "id": "3Ja22xZuFZLk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, GPT2Tokenizer, pipeline\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./gpt2-news-merged\") # load model with using AutoTransformer class\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") # Tokenizer is same GPT2's\n",
        "\n",
        "# generate generator with using pipeline\n",
        "generator = pipeline(\"text-generation\", # task: text-generation\n",
        "                     model=model, # model = gpt2-news-merged\n",
        "                     tokenizer=tokenizer) # translate `input_ids` into natural language. with using this tokenizer(GPT2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJF613zRJfbK",
        "outputId": "abbe8c12-a9ed-4c3a-fc30-ce534e817c90"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Breaking News\" # prompt. model will be generating from this sentence\n",
        "\n",
        "outputs = generator(prompt, # prompt\n",
        "                    max_length=128, # max token length\n",
        "                    num_return_sequences=3, # number of generting sequence\n",
        "                    do_sample=True, # sampling true\n",
        "                    top_k=50) # sampling only for top 50 vector\n",
        "\n",
        "# print generated outputs\n",
        "for i, out in enumerate(outputs):\n",
        "    print(f\"--- Generated #{i+1} ---\")\n",
        "    print(out[\"generated_text\"])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2UH_qfaKRg9",
        "outputId": "b9a36a9c-7b65-430b-e3e6-2c1c9dfc7c2d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generated #1 ---\n",
            "Breaking News: New York Times Is In The Business Of Selling Paper for Profit\n",
            "\n",
            "--- Generated #2 ---\n",
            "Breaking News: A New York Times report from the United States indicates that President Barack Obama will be returning to his post as Secretary of State in his first week in office.\n",
            "\n",
            "--- Generated #3 ---\n",
            "Breaking News & Rumors A report from The Daily Mail has claimed that former Conservative MP David Cameron has been suspended from the party after a controversial speech in which he accused the party of being anti-Islamic.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 vs GPT-2**News**"
      ],
      "metadata": {
        "id": "3rhHs-pBQr0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, AutoModelForCausalLM, GPT2Tokenizer, pipeline\n",
        "\n",
        "# ---------- GPT2 ------------\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# ---------- GPT2News ----------\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./gpt2-news-merged\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2news_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# --------- Test --------------\n",
        "prompt = \"Seoul, South Korea\"\n",
        "\n",
        "gpt2_outputs = gpt2_generator(prompt, max_length=128, num_return_sequences=3, do_sample=True, top_k=50)\n",
        "gpt2news_outputs = gpt2news_generator(prompt, max_length=128, num_return_sequences=3, do_sample=True, top_k=50)\n",
        "\n",
        "# 1. 원본 GPT-2의 생성 결과 출력\n",
        "print(\"=============================================\")\n",
        "print(\"          🤖 Original GPT-2 Results          \")\n",
        "print(\"=============================================\")\n",
        "print(f\"PROMPT: '{prompt}'\\n\")\n",
        "\n",
        "for i, output in enumerate(gpt2_outputs):\n",
        "    print(f\"--- Generated #{i+1} ---\")\n",
        "    # 파이프라인 결과는 딕셔너리 형태이므로 'generated_text' 키로 접근합니다.\n",
        "    print(output['generated_text'])\n",
        "    print() # 줄바꿈\n",
        "\n",
        "# 2. 파인튜닝된 GPT-2 News 모델의 생성 결과 출력\n",
        "print(\"\\n=============================================\")\n",
        "print(\"      ✨ Fine-tuned 'GPT-2 News' Results      \")\n",
        "print(\"=============================================\")\n",
        "print(f\"PROMPT: '{prompt}'\\n\")\n",
        "\n",
        "for i, output in enumerate(gpt2news_outputs):\n",
        "    print(f\"--- Generated #{i+1} ---\")\n",
        "    print(output['generated_text'])\n",
        "    print() # 줄바꿈"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t22znEjQyO7",
        "outputId": "c42da38d-ec75-4647-dfa6-30907367d090"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================\n",
            "          🤖 Original GPT-2 Results          \n",
            "=============================================\n",
            "PROMPT: 'Seoul, South Korea'\n",
            "\n",
            "--- Generated #1 ---\n",
            "Seoul, South Korea, July 31, 2017. REUTERS/Kim Hong-Ji\n",
            "\n",
            "The new law comes as Beijing and Seoul have increasingly focused on the so-called \"comfort women\" - women who are forced to perform sex acts under pressure from their husbands.\n",
            "\n",
            "The South Korean government said the legislation had been drafted over the past two years and aimed not only at the most vulnerable, but also those with pre-existing mental health problems.\n",
            "\n",
            "\"The law should be used to punish those who have such disabilities. We are also concerned about the psychological impact on their families,\" a ministry statement said.\n",
            "\n",
            "\"We will use it as a deterrent against those who dare to call themselves 'comfort women,'\" it said.\n",
            "\n",
            "Many men have been forced to perform sex acts under pressure from their wives, who are not protected under the law, according to the rights group.\n",
            "\n",
            "In January, North Korea's ministry of women's affairs published a series of guidelines in which it said it would not allow any man to give the women a \"secret\" \"comfort plane\" and would not provide any information about them.\n",
            "\n",
            "It also said the information would be \"deprived of the sense of dignity of the women.\"\n",
            "\n",
            "\"The women will be held accountable as they\n",
            "\n",
            "--- Generated #2 ---\n",
            "Seoul, South Korea. The North Korean government has said it will stop its ballistic missile development next year, and the country has also said it has agreed to suspend its missile test program until a new set of tests are conducted.\n",
            "\n",
            "North Korea's leader Kim Jong Un's government has said his country will conduct a \"nuclear test\" next year, and the country has said it will be able to conduct more tests. (AP Photo/Kim Kyung-Hoon)\n",
            "\n",
            "The North Koreans have said they will continue to test and test, but have repeatedly said they will not use force to prevent the North from launching its fourth nuclear test, a day after it tested a hydrogen bomb.\n",
            "\n",
            "In contrast to such statements, North Korea has said in the past that it is prepared to use its nuclear weapons to take revenge on its neighbors and will not abandon its nuclear program.\n",
            "\n",
            "The United States has said it is ready to defend itself and the region against any North Korean regime that threatens its territory or \"threatens to test our national security.\"\n",
            "\n",
            "The North Korean government has also said it would conduct a \"nuclear test\" if it is attacked by North Korea.\n",
            "\n",
            "The United States has said it will not attack North Korea if the country attacks the United States.\n",
            "\n",
            "\n",
            "\n",
            "--- Generated #3 ---\n",
            "Seoul, South Korea, on Nov. 14, 2017. REUTERS/Kim Hong-Ji\n",
            "\n",
            "The South Korean government is considering a measure to ban the sale of marijuana, although it would take effect no later than next month, South Korea's Ministry of Youth Affairs said in a statement.\n",
            "\n",
            "The measure is seen as a step toward limiting the availability of illicit drugs and increasing the legal system.\n",
            "\n",
            "Last year, South Korea banned retail marijuana shops and began allowing residents to get pot from a source outside of a city that has been decriminalized in recent years.\n",
            "\n",
            "But legalization has sparked a crackdown on street vendors and drug users who use drugs in a country that has seen a series of gruesome murders and disappearances.\n",
            "\n",
            "\"The government's decision to ban the sale of marijuana comes after an international outcry against the illegal sale of marijuana, and has led to criminalization of many street vendors and criminals,\" said Yeonjung Kim, director of the Korea Institute for Research on Drugs and Public Health.\n",
            "\n",
            "\"While the law may not have been enacted to stop the illegal sale of marijuana, it is clear that the government is not going to allow the sale of marijuana to people who are not authorized to buy it,\" Choi told Reuters.\n",
            "\n",
            "\n",
            "=============================================\n",
            "      ✨ Fine-tuned 'GPT-2 News' Results      \n",
            "=============================================\n",
            "PROMPT: 'Seoul, South Korea'\n",
            "\n",
            "--- Generated #1 ---\n",
            "Seoul, South Korea (Reuters) - South Korean President Park Geun-hye's visit on Thursday to Seoul to mark the 50th anniversary of the end of the Korean War was greeted by cheers from tens of thousands of South Koreans and South Korean officials, as she made her first public visit to South Korea since resigning in disgrace in December.\n",
            "\n",
            "--- Generated #2 ---\n",
            "Seoul, South Korea, South Korea, South Korea - South Korea's Prime Minister Lee Myung-bak (L) shakes hands with a South Korean delegation during a meeting in Seoul, South Korea    on Thursday.\n",
            "\n",
            "--- Generated #3 ---\n",
            "Seoul, South Korea: South Koreans celebrate by firing missiles at a military base South Korea on Friday (AFP) - South Korea, the country of South Korea, released pictures of the dead and wounded at a news conference attended by a South Korean president and a South Korean army chief on Friday.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hl60BfidUwBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
